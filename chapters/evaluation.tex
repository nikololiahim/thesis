\chapter{Evaluation and Discussion}
\label{chap:eval}
This chapter provides the evaluation of the resulting implementation. Section \ref{eval:limitations} outlines the limitations of the EO-based static analysis. Section \ref{eval:testing} describes how the analyzers were tested. Finally, section \ref{eval:comparisons} describes the result of comparing EO-based static analyzers with their counerparts for other programming languages.

\section{Limitations}
\label{eval:limitations}

\subsection{General}
The analyzer provided minimal information about the location of discovered errors. This is especially important if the analyzer is to be used in the integrated development environments.

The analyzer only works on single files. If EO objects are spread across multiple files, the current implementation would be able to analyze only one file at a time. There is a minimal support for objects imported from other EO files, i.e. analyzer acknowledges their existence and does not report them as missing. However, bodies of object imported from other files can not be accessed, therefore no meaningful analysis can be performed if one of the used objects comes from another file.

\subsection{Unanticipated Mutual Recursion}
The implementation of does not do any path-dependent analysis, therefore it may produce false-positives in case when the call to the mutually-recursive method is unreachable, e.g. when guarded by a statement which can only be false.

\subsection{Unjustified Assumption in Subclass}
The analyzer relies on the external SMT-solver called Princess \cite{princess}. The solver has some limitations when it comes to the support of SMTLIB-2 \cite{smtlib} format. A lot of effort was spent working around the peculiarities of the solver interface. This limits the portability of the implementation if a different SMTLIB backend is to be chosen.

The current implementation of the analyzer supports only a limited set of types. This limitation is directly imposed by the lack of a static type checker in EO. Specifically, all method parameters (excluding \textbf{self}) and their return methods are assumed to be of integer numeric type. This imposes significant limitations on the type of constraints that can be decided by the solver.

Finally, the complexity of the constraints the solver needs to decide grows linearly with the size of the program. While the complexity of the solver operation is largely unknown, it would be safe to estimate that the execution time of the analyzer would be rather slow on the large programs with a lot of constraints.

\section{Testing}
\label{eval:testing}
The implementation is largely covered by hand-written integration tests. For mutual-recursion, the property-based testing technique \cite{property_based_testing} was also applied to ensure the extensive coverage of the input domain.

\subsection{Integration Testing}
\label{eval:integration_testing}
Integration testing or end-to-end testing \cite[Chapter 7]{testing} describes an approach to testing when one testcase covers the functionality of the entire software system. In our case, the modules under tests where the analyzers, the input was the EO code with or without the respective defects and the expected output was the error messages produced by the respective analyzer, which is represented by a list of strings.

To execute the tests we used a testing framework for Scala called Scalatest \footnote{\url{https://www.scalatest.org/}}. To represent the individual test case we used a the following case class definition:

\begin{lstlisting}[language=Scala]
case class TestCase(
    label: String,
    code: String,
    expected: List[String]
  )
    \end{lstlisting}

The $label$ is a short description of the test case used mostly for human readability. $code$ field holds the code to be analyzed, while the $expected$ field holds the errors that are supposed to be detected by the analyzer in the $code$ field.

All the test cases for a particular analyzer are divided into two groups: one for test cases where the input code contains errors, the other for test cases where the input code does not contain errors. These groups are each represented by a testsuite-local variable of type \textbf{List[TestCase]}:
\begin{lstlisting}[language=Scala]
    val testCasesWithErrors: List[TestCase] = 
        List(casewithErrors1, ..., caseWithErrorsN)
    val testCasesWithoutErrors: List[TestCase] = 
        List(caseWithoutErrors1, ..., caseWithoutErrorsN)
\end{lstlisting}

Each test group is then run using a driver function called $runTests$. This function registers the test in the Scalatest test suite, executes the analyzer on the input code to obtain the errors and then compares the obtained errors with the expected using the assert statement:
\begin{lstlisting}[language=Scala]
    
def runTests(tests: List[TestCase]): Unit =
  tests.foreach { 
    case TestCase(label, code, expected) =>
      registerTest(label) {
        val obtained = analyze(code).unsafeRunSync()
        assert(obtained == expected)
      }
  }

\end{lstlisting}

\subsection{Property-based Testing}
Property-based testing is a variation of random testing that can be used when there exists a set of well-defined properties (or predicates) that should be satisfied for a set of possible inputs of the function being tested.  This predicate is then being evaluated on the randomly generated input data. The key difference between property-based testing and the conventional testing approach, like the one described in \ref{eval:integration_testing}, is the fact that fact that the random input data is generated \textit{automatically}, which covers a large effective subset of the input domain that would otherwise be impractical or impossible to perform manually. 

\section{Comparisons}
\label{eval:comparisons}
